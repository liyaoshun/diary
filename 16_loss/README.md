# **LOSS相关整理**
[LINK](https://mp.weixin.qq.com/s/X2B68OfeOozpUKAAieBm8A)
```
首先损失函数的功能是通过样本来计算模型分布与目标分布间的差异，在分布差异计算中，KL 散度是最合适的。但在实际中，某一事件的标签是已知不变的（例如我们设置猫的 label 为 1，那么所有关于猫的样本都要标记为 1），即目标分布的熵为常数。而根据下面 KL 公式可以看到，KL 散度 - 目标分布熵 = 交叉熵（这里的“-”表示裁剪）。所以我们不用计算 KL 散度，只需要计算交叉熵就可以得到模型分布与目标分布的损失值。

从上面介绍，知道了模型分布与目标分布差异可用交叉熵代替 KL 散度的条件是目标分布为常数。如果目标分布是有变化的（如同为猫的样本，不同的样本，其值也会有差异），那么就不能使用交叉熵，例如蒸馏模型的损失函数就是 KL 散度，因为蒸馏模型的目标分布也是一个模型，该模型针对同类别的不同样本，会给出不同的预测值（如两张猫的图片 a 和 b，目标模型对 a 预测为猫的值是 0.6，对 b 预测为猫的值是 0.8）。

注：交叉熵和 KL 散度应用方式不同的另一种解释（我更倾向于上面我自己的解释，更具公式解释性）：

交叉熵：其用来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小。这也是为什么在机器学习中的分类算法中，我们总是最小化交叉熵，因为交叉熵越低，就证明由算法所产生的策略最接近最优策略，也间接证明我们算法所算出的非真实分布越接近真实分布。

KL 散度（相对熵）：衡量不同策略之间的差异呢，所以我们使用 KL 散度来做模型分布的拟合损失。

```

## **信息量**

```
任何事件都会承载着一定的信息量，包括已经发生的事件和未发生的事件，只是它们承载的信息量会有所不同。如昨天下雨这个已知事件，因为已经发生，既定事实，那么它的信息量就为 0。如明天会下雨这个事件，因为未有发生，那么这个事件的信息量就大。

从上面例子可以看出信息量是一个与事件发生概率相关的概念，而且可以得出，事件发生的概率越小，其信息量越大。这也很好理解，狗咬人不算信息，人咬狗才叫信息嘛。

我们已知某个事件的信息量是与它发生的概率有关，那我们可以通过如下公式计算信息量：

假设X是一个离散型随机变量，其取值集合为Y,概率分布函数p(x)=Pr(X=x),x的定义域为Y,则定义事件X=x0的信息量为：I(x0)=-log(p(x0))  
```

## **熵**

```
我们知道：当一个事件发生的概率为p(x)，那么它的信息量是-log(p(x))。那么如果我们把这个时间的所有可能性都罗列出来，就可以求得该事件信息量的期望，信息量的期望就是熵，所以熵的公式为：
假设 事件X共有n种可能，发生xi的概率为p(xi)，那么该事件的熵H(x)为：

H(x) = -sum(p(xi)log(p(xi))) i~[1, n]

有一类比较特殊的问题，比如抛硬币只有两种可能，字向上或者花向上，叫这类问题为0-1分布问题(二项分布的特例)，对于这类问题，熵的计算方法可以简化为如下算式：

H(x)=-p(x)log(p(x))-(1-p(x))log(1-p(x))

```

## **相对熵（KL 散度）**
```
相对熵又称 KL 散度,如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可以使用 KL 散度（Kullback-Leibler (KL) divergence）来衡量这两个分布的差异。
在机器学习中，P 往往用来表示样本的真实分布，Q 用来表示模型所预测的分布，那么 KL 散度就可以计算两个分布的差异，也就是 Loss 损失值。

Dkl(p||q)=sum(p(xi)log(p(xi)/q(xi))) i~[1,n]

从 KL 散度公式中可以看到 Q 的分布越接近 P（Q 分布越拟合 P），那么散度值越小，即损失值越小。因为对数函数是凸函数，所以 KL 散度的值为非负数。有时会将 KL 散度称为 KL 距离，但它并不满足距离的性质：
   1. KL 散度不是对称的；
   2. KL 散度不满足三角不等式。

```

## **交叉熵**
```
我们将 KL 散度公式进行变形,通过整理后可以将KL散度用熵和交叉熵来表示，

Dkl(p||q)=-H(p(x)) + [-sum(p(xi)log(q(xi)))] x~[1,n]

所以交叉熵公式如下：

H(p,q)=-sum(p(xi)log(q(xi)))  x~[1,n]

在机器学习中，我们需要评估 label 和 predicts 之间的差距，使用 KL 散度刚刚好，即Dkl(y||y`)，由于 KL 散度中的前一部分−H(y)不变，故在优化过程中，只需要关注交叉熵就可以了。所以一般在机器学习中直接用用交叉熵做 loss，评估模型。

```


## **JS 散度**
```
JS 散度度量了两个概率分布的相似度，基于 KL 散度的变体，解决了 KL 散度非对称的问题。一般地，JS 散度是对称的，其取值是 0 到 1 之间。定义如下：
JS(p1||p2)=(1/2)KL(p1||(p1+p2)/2)+(1/2)KL(p2||(p1+p2)/2)
```