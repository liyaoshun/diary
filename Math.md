# <div align = center>**数学相关知识** </div>

## **1. 极大似然估计与最大后验概率估计**
[知乎](https://zhuanlan.zhihu.com/p/184028576)

频率学派-极大似然估计（MLE） | 极大似然估计方法（Maximum Likelihood Estimate，MLE）也称为最大概似估计或最大似然估计
```
他们认为模型参数是个定值，希望通过类似解方程组的方式从数据中求得该未知数,这种方法往往在大数据量的情况下可以很好的还原模型的真实情况。

最大似然估计，只是一种概率论在统计学的应用，它是参数估计的方法之一。说的是已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，参数估计就是通过若干次试验，观察其结果，利用结果推出参数的大概值。最大似然估计是建立在这样的思想上：已知某个参数能使这个样本出现的概率最大，我们当然不会再去选择其他小概率的样本，所以干脆就把这个参数作为估计的真实值。

其求解步骤：
1. 写出似然函数
2. 同时对似然函数两边取对数。（Ln）
3. 对seta预估参数求导，然后使其等于0。最后求出参数seta的预估值。

通过取自然对数（Ln），可以降低似然函数L(θ)中因变量θ的复杂度，方便求解。如果直接对似然函数L(θ)取导并使其为0，并求得使L(θ)取得最大值的θ值，也可以的，但是有可能L(θ)中θ太复杂而导致计算复杂度很高。
```
贝叶斯派-最大后验概率估计（MAP）
```
他们认为世界是不确定的，因获取的信息不同而异。假设对世界先有一个预先的估计，然后通过获取的信息来不断调整之前的预估计。 他们不试图对事件本身进行建模，而是从旁观者的角度来说。因此对于同一个事件，不同的人掌握的先验不同的话，那么他们所认为的事件状态也会不同。

他们认为模型参数源自某种潜在分布，希望从数据中推知该分布。对于数据的观测方式不同或者假设不同，那么推知的该参数也会因此而存在差异。这种方法在先验假设比较靠谱的情况下效果显著，随着数据量的增加，先验假设对于模型参数的主导作用会逐渐削弱，相反真实的数据样例会大大占据有利地位。极端情况下，比如把先验假设去掉，或者假设先验满足均匀分布的话，那她和极大似然估计就如出一辙了。
```

极大似然估计与最大后验概率估计
```
我们这有一个任务，就是根据已知的一堆数据样本，来推测产生该数据的模型的参数，即已知数据，推测模型和参数。因此根据两大派别的不同，对于模型的参数估计方法也有两类：极大似然估计与最大后验概率估计。
```


**MAP与MLE最大区别是MAP中加入了模型参数本身的概率分布，或者说，MLE中认为模型参数本身的概率的是均匀的，即该概率为一个固定值。MAP允许我们把先验知识加入到估计模型中，这在样本很少的时候是很有用的，因为样本很少的时候我们的观测结果很可能出现偏差，此时先验知识会把估计的结果“拉”向先验，实际的预估结果将会在先验结果的两侧形成一个顶峰。通过调节先验分布的参数，比如beta分布的，我们还可以调节把估计的结果“拉”向先验的幅度，越大，这个顶峰越尖锐。这样的参数，我们叫做预估模型的“超参数”。**

**MAP与Bayesian区别：尽管最大后验估计与 Bayesian 统计共享前验分布的使用，通常并不认为它是一种 Bayesian 方法**


<div  align=center>
<img src="images/mle_map.png">
</div>



## **2. 信息量，信息熵，交叉熵，KL散度和互信息（信息增益）**

### **信息量**
信息量用一个信息所需要的编码长度来定义,而一个信息的编码长度跟其出现的概率呈负相关,因为一个短编码的代价也是巨大的,因为会放弃所有以其为前缀的编码方式,比如字母”a”用单一个0作为编码的话,那么为了避免歧义,就不能有其他任何0开头的编码词了.所以一个词出现的越频繁,则其编码方式也就越短,同时付出的代价也大.

$$I = log_2(\frac{1}{p(x)}) = -log_2(p(x))$$

### **信息熵**

而信息熵则代表一个分布的信息量,或者编码的平均长度

$$H(p) = \sum_x p(x)\log_2\left(\frac{1}{p(x)}\right) = -\sum_x p(x)\log_2\left(p(x)\right)$$
即信息量的均值

### **交叉熵 cross-entropy**
交叉熵本质上可以看成,用一个猜测的分布的编码方式去编码其真实的分布,得到的平均编码长度或者信息量
$$H_p(q) = \sum_x q(x)\log_2\left(\frac{1}{p(x)}\right)$$

如上面的式子,用猜的的p分布,去编码原本真是为q的分布,得到的信息量

### **交叉熵 cross-entropy在机器学习领域的作用**
交叉熵cross-entropy在机器学习领域中经常作为最后的损失函数
为什么要用cross-entropy呢，他本质上相当于衡量两个编码方式之间的差值，因为只有当猜测的分布约接近于真实分布，则其值越小。
比如根据自己模型得到的A的概率是80%，得到B的概率是20%，真实的分布是应该得到A，则意味着得到A的概率是100%，所以

$$L = -\sum_iy_ilog(p(x_i))+(1-y_i)log(1-p(x_i))$$

### **KL散度**

$$D_q(p) = H_q(p) - H(q) = \sum_x q(x)\log_2\left(\frac{q(x)}{p(x)}\right)$$

### **互信息（信息增益）**
互信息就是一个联合分布中的两个信息的纠缠程度/或者叫相互影响那部分的信息量

$$I(X,Y) = H(X) + H(Y) - H(X,Y)$$
$$I(X,Y) = H(Y) - H(Y|X)$$

决策树中的信息增益就是互信息，决策树是采用的上面第二种计算方法，即把分类的不同结果看成不同随机事件Y，然后把当前选择的特征看成X，则信息增益就是当前Y的信息熵减去已知X情况下的信息熵。